program: train_dynaq.py
method: bayes  
metric:
  name: episode/avg_reward
  goal: maximize
parameters:
  # --- Standard RL Params ---
  batch_size:
    values: [32, 64, 128]
  
  # --- Dyna-Q Specific Params ---
  # The ratio of real data in a batch (e.g., 0.5 means 50% real, 50% dreamed)
  q_batch_ratio:
    distribution: uniform
    min: 0.2
    max: 0.8
  
  # batch size to update the world model
  dream_batch_size:
    values: [16, 32, 64, 128]

  imagined_buffer_size:
    values: [50000, 100000, 200000]

  # How often (in steps) to generate dreams
  dream_frequency:
    values: [8, 16, 32]
    
  # Learning rate for the world model (usually higher or equal to agent LR)
  model_learning_rate:
    values: [0.001, 0.0001]
    
  # When to start dreaming (warmup period)
  dream_start_episode:
    values: [20, 100, 500]

  dream_switch_off_episode:
    values: [250, 400, 600]

  # how many past episodes to use to update the world model
  model_update_episodes:
    values: [3, 5, 10]

  avg_episode_length:
    values: [200, 400, 600]

command:
  - ${env}
  - python3
  - ${program}
  - ${args}
  - "--total-episodes"
  - "600" # Shorten episodes for faster tuning, then train full on best params